{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from fieldfinder import SpectralIndex #, write_NDVI_mask\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PlanetScope Ortho Scene Product (3B)](https://notebook.community/planetlabs/notebooks/jupyter-notebooks/toar/toar_planetscope)\n",
    "MS radiance\n",
    "\n",
    "Planet Labs' analytic data products are reported in units of radiance: $W*m^{-2}*sr^{-1}$\n",
    "\n",
    "That means that every pixel in an analytic tiff has a natural language interpretation: \"How much light was captured over this spot of ground?\"\n",
    "\n",
    "But over the course of a day or year, the number of photons that the sun shines on the scene rises and falls. If you naively compare the radiance values of two scenes over the same spot on Earth from the same satellite but a few weeks (or even just hours!) apart, you will likely find dramatic radiance differences even if nothing on the ground has changed!\n",
    "\n",
    "In addition to this variation, each of Planet Labs' 150+ satllites have small amounts of variation in their spectral filters which yields slight differences in radiance measurements, even from two satellites taking pictures of the same exact place at the same exact moment!\n",
    "\n",
    "Top of Atmosphere Reflectance is extremely useful because it is an apples-to-apples comparable number from any satellite over any location that does not change with time of day or time of year unless the content of the scene changes. It is very commonly used in GIS applications which compute spectral indices such as NDVI or NDWI.\n",
    "\n",
    "$$ \\begin{equation*} \\mathbf{img}_{reflectance} = \\begin{vmatrix} a \\\\ b \\\\ c \\\\ d \\end{vmatrix} \\times \\mathbf{img}_{radiance} \\end{equation*} $$\n",
    "The four coefficients $a, b, c, d$ are calculated and provided with every analytic image that Planet provides and can be used as simple scaling factors to convert from radiance to reflectance. Their values change with the image's local time of day and time of year, and do so uniquely per satellite.\n",
    "\n",
    "**Most spectral indices are defined in terms of reflectance, not radiance.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what is the actual data in the raw image?\n",
    "- can we get the coefficients from anywhere?\n",
    "\n",
    "From the PlanetScope definition page:\n",
    "As a last step, the spatially and temporally adjusted datasets are transformed from digital number values into physical based radiance values (scaled to W * m²strμm)*100)\n",
    "\n",
    "most definitions of NDVI seem to be based on reflectance values.  The data provided are radiance values.  Need the metadata to get the coefficients to turn into reflectance values.\n",
    "\n",
    "from: https://eoscience.esa.int/landtraining2017/files/materials/D4P1b2I.pdf\n",
    "> To calculate NDVI we have to convert radiance to reflectance\n",
    "\n",
    "from: https://gis.stackexchange.com/questions/171453/is-the-reflectance-required-to-get-the-ndvi-for-landsat-8-images\n",
    "> NDVI is defined for any two bands with near-infrared and infrared data (it is an empirical remote sensing index). As such, you can calculate it straight from the DNs. This is mostly OK if you are only classifying or analyzing vegetation on a single image without significant atmospheric effects (cirrus clouds...)\n",
    "\n",
    "> However, if you are performing change detection (and therefore analysing two or more images), you will need to cope with two steps:\n",
    "\n",
    "> Radiometric calibration converts the DNs to radiance using sensor-specific calibration equations. For Landsat, these can be found on the project website or built in most of the good software packages.\n",
    "\n",
    "> Atmospheric correction tries to convert the radiance to reflectance (a dimensionless number describing the ratio of reflected radiation on the incoming radiation in the specific part of the spectrum). For this, atmosphere models are used such as QUAC or FLAASH.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://assets.planet.com/docs/Planet_Combined_Imagery_Product_Specs_letter_screen.pdf\n",
    "\n",
    "What's in the provided data file?\n",
    "File name format for PlanetScope Ortho Scene Product (3B)\n",
    "YYYYMMDD_UTC-TIME_SAT-ID_PRODLVL_PRODTYPE\n",
    "\n",
    "\"<acquisition date>_<acquisition time>_<satellite_id>_<productLevel><bandProduct>.<extension>\"\n",
    "\n",
    "So this image was captured on August 27, 2021 at 10 am (Central Time).  The satellite ID is 60.  2262 might be a tile ID?  \n",
    "\n",
    "3B is the product level.  This is the 'analytic ortho-scene product'. From the product description:\n",
    "\n",
    ">Orthorectified, scaled Top of Atmosphere Radiance (at sensor) or Surface Reflectance image product suitable for analytic and visual applications. This product has scene based framing and projected to a cartographic projection.\n",
    "\n",
    "Analytic MS is the product type. 8b stands for 8 band.\n",
    "\n",
    "This is from a PlanetScope SuperDove satellite, which produces 8-band images.\n",
    "\n",
    "Coastal Blue 431-452 nm\n",
    "Blue: 465-515 nm\n",
    "Green I: 513. - 549 nm\n",
    "Green: 547. - 583 nm\n",
    "Yellow: 600-620 nm\n",
    "Red: 650 - 680 nm\n",
    "Red-Edge: 697 - 713 nm\n",
    "NIR: 845 - 885 nm\n",
    "\n",
    "\n",
    "from https://developers.planet.com/docs/data/planetscope/:\n",
    "Analytic (analytic) assets are orthorectified, calibrated, multispectral imagery products that have been corrected for sensor artifacts and terrain distortions, and transformed to Top of Atmosphere (at-sensor) radiance.\n",
    "\n",
    "https://www.l3harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/16278/Digital-Number-Radiance-and-Reflectance\n",
    "Typically, for quantitative analysis of multispectral or hyperspectral image data, radiance images are corrected to reflectance images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Rasterio\n",
    "\n",
    "https://developers.planet.com/docs/planetschool/calculate-an-ndvi-in-python/\n",
    "https://www.earthdatascience.org/courses/use-data-open-source-python/multispectral-remote-sensing/vegetation-indices-in-python/calculate-NDVI-python/\n",
    "\n",
    "Rabbit Hole!\n",
    "https://yceo.yale.edu/how-convert-landsat-dns-top-atmosphere-toa-reflectance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WGS 84 / UTM zone 15N](https://epsg.io/32615)\n",
    "\n",
    "Output to EPSG 4326, or WGS 84 - Lat/Long coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "136 million pixels in each image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI is the normalized difference between the RED and NIR values\n",
    "$$NDVI = \\frac{NIR - R}{NIR + R}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spectral Index (NDVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/example_image.tif'\n",
    "\n",
    "ndvi = SpectralIndex(filename)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(ndvi.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(ndvi.get_mask(0.65))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_proj = 'EPSG:4326'\n",
    "output_file = '/Users/brendon/Projects/challenges/farmfinder/data/transformed_example_module.tif'\n",
    "\n",
    "ndvi.write_mask(output_file, threshold=0.65, out_proj=out_proj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience function for creating a mask file with a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/example_image.tif'\n",
    "threshold = 0.65\n",
    "out_proj = 'EPSG:4326'\n",
    "output_file = '/Users/brendon/Projects/challenges/farmfinder/data/transformed_example_single_single_from_class.tif'\n",
    "SpectralIndex.create_mask_file(filename, output_file, threshold, out_proj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "660830de0d491fbddd687273b325eac1078f242d274501bfeaf11c59223703c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
